---
title: "Week 9 Lab:  Time Series Data and Analysis Strategies"
author: "Elena Austin for ENVH 556"
date: "Winter 2021; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        df_print: "paged"
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
---

```{r setup, include=FALSE}
#-----setup-----

# set knitr options
knitr::opts_chunk$set(echo = TRUE)

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
    rm(res)
}

```

```{r load.libraries, echo=FALSE, include=FALSE, eval=TRUE}
#-----load libraries-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, dplyr, tidyr, data.table, modelr, stringr, ggplot2, 
               lubridate)

# load functions from function library
source(file.path("function_library", "read_data.R"))
source(file.path("function_library", "get_external_data.R"))

```

```{r directory.organization.read.data, echo=FALSE, warning=FALSE}
#-----directory organization and read data-----

# specify working directory
project_path <- getwd()

# specify data directory
data_dir <- file.path(project_path, "Datasets", "Summer'18")

```

# Introduction and Purpose

The purpose of this lab is to practice working with time series data, develop
facility in importing and merging data of varying time scales, smoothing
approaches, and some analysis tools to reduce the complexity of these data. In
addition, as with all the labs in this course, you should focus on writing up a
coherent lab report that addresses the scientific context and scientific
questions that can be answered using variance components.

In this lab exercise we will use data collected as part of the MOV-UP mobile
monitoring campaign. This data was collected on 1 second and 10 second time
scales. The lab will walk you through the steps of important the raw instrument
data. Instruments included in the lab include a PTrak particle counting
instrument, a CPC instrument, a GPS tracker, a black carbon aethelometer and a
NanoScan particle sizing spectrometer. The full suite of instruments and their
characteristics of the platform used in this study can be found in [Xiang et
al., 2020](https://doi.org/10.1021/acs.est.0c00612).

# Getting Started

Managing dates and times in exposure assessment is a common challenge because
there are a variety of common date and time formats used to log instrument data.
You've no doubt encountered the following representations of dates:

   * `r format(Sys.time(), '%d %b, %Y')`
   * `r format(Sys.time(), '%m/%d/%Y')`
   * `r format(Sys.time(), '%Y-%m-%d')`

With direct-reading instruments made all over the world, you'll have the
potential to encounter many devices that use different date and time formats.
You'll find it advantageous to convert everything to a standardized format,
including the time zone. Consistently using a standardized date and time format
will make it less likely that you'll interpret a date and time incorrectly.

We suggest the International Organization for Standardization (ISO) date and
time standard, which provides an unambiguous, well-known, and well-defined
method of representing dates and times. Dates are often not stored in ISO 8601
date format, making it necessary to define date/time components specifically for
each instrument. In addition, the time scale of each instrument can be
different, and it is necessary to ensure a common time base prior to merging the
data.

# Identify Files of Interest

```{r gather.file.names.for.import}
#-----gather file names for import-----

# create an input list, specifying input data characteristics
input <- list(user_time_avg = 1, #30, 60, 60*5 
              data_options = c("missing", "ksea") )

# get filename paths for data files, 
   # "recursive" means the function will dig into subdirectories
   # subset to file names that do not contain "CoLo" or "Fixed Sites" 
filename_paths = list.files(data_dir, recursive=T, full.names=T) %>% 
   str_subset("CoLo|Fixed Sites", negate = TRUE)

# [Elena, I didn't see any files with "CoLo" or "Fixed Sites" in the names,
# commented out for now]
#filename_paths = grep("CoLo", filename_paths, value=T, invert=T)
#filename_paths = grep("Fixed Sites", filename_paths, value=T, invert=T)


# Get GPS file names 
gps_path = str_subset(filename_paths, "GPS")
gps_file = basename(gps_path)


# get p-trak path
ptrak_path = str_subset(filename_paths, "PT")

# identify non-screened files and get file names
ptrak_no_screen <- ptrak_path %>% 
   str_subset("scrnd|Scrnd|Screened|screen", negate = TRUE)

ptrak_no_screen_file = basename(ptrak_no_screen)


# identify screened files and get file names
ptrak_screen <- ptrak_path %>% 
   str_subset("scrnd|Scrnd|Screened|screen")
   
ptrak_screen_file = basename(ptrak_screen)

# identify AE51 path and file
ae51_path = str_subset(filename_paths, "AE51")
ae51_file = basename(ae51_path)

# identify cpc path and file
cpc_path = str_subset(filename_paths, "CPC")
cpc_file = basename(cpc_path)

# get scan path and file
scan_path = str_subset(filename_paths, "Scan")
scan_file = basename(scan_path)

# get CO path and file
co2_path = str_subset(filename_paths, "CO2")
co2_file = basename(co2_path)

# get nano single path and file
# [Elena, I don't see any nano files in these folders]
# nano_single_path <- filename_paths %>% 
#    str_subset("Size") %>% 
#    str_subset("VOID", negate = TRUE)
# 
# nano_single_file <- basename(nano_single_path)

```

# Read Data Files

```{r read.data}
#-----read data-----

# Initialize all data sets to NULL 
# [CZ: I don't think this is necessary with lapply]
   # gps.data = NULL
   # langan.data = NULL
   # ptrak.data = NULL
   # ptrak.screen.data =NULL
   # ae51.data= NULL
   # cpc.data=NULL
   # nanoScan.data = NULL
   # nanoSingle.data = NULL
   # Labview.data=NULL
   # filelog.data =NULL
   # weather_data = NULL
   # co2.data = NULL


# read in GPS data      
gps.data <- lapply(seq_along(gps_path), FUN = function(fileind) {
  #read gps file and apply a time averaging based on the input parameters
  read.gps(datafile = gps_path[fileind],
           runname = getrunname(gps_file[fileind]),
           location = "mobile",
           timeaverage = as.numeric(input$user_time_avg)/60,
           splineval= "missing" %in% input$data_options) 
}) %>% 
   # bind list objects
   bind_rows() %>% 

   # set data.table keys
   setkey(timeint, runname, location)


# read in non-screened p-trak data
ptrak.data <- lapply(seq_along(ptrak_no_screen), FUN = function(fileind) { 
   try(read.ptrak(datafile = ptrak_no_screen[fileind],
                  runname = getrunname(ptrak_no_screen_file[fileind]),
                  timeaverage = as.numeric(input$user_time_avg)/60,
                  location = "mobile",
                  screen = F,
                  splineval= "missing" %in% input$data_options) )
}) %>% 
   # bind rows
   bind_rows() %>%

   # average by the time interval of interest
   # [CZ: this doesn't appear to do anything as written...]
   #ptrak.data = ptrak.data[, lapply(.SD, mean) , 
   #                      by = c("timeint","runname", "location", "serial.noscreen")]

   # set data.table key
   setkey(timeint, runname, location, serial.noscreen)


# read screened p-trak data 
ptrak.screen.data <- lapply(seq_along(ptrak_screen), FUN = function(fileind) { 
   
   read.ptrak(datafile=ptrak_screen[fileind],
              runname = getrunname(ptrak_screen_file[fileind]),
              location = "mobile",
              timeaverage = as.numeric(input$user_time_avg)/60,
              screen=T,
              splineval= "missing" %in% input$data_options )
}) %>% 
   # bind list together 
   bind_rows() %>% 
   
   # # average? [CZ: same here?]
   # ptrak.screen.data = ptrak.screen.data[, lapply(.SD, mean) , 
   #                                 by = c("timeint","runname", 
   #                                        "location", "serial.screen")]
   
   # set data.table keys
   setkey(timeint, runname, location, serial.screen)


# read in CO2 data
co2.data <- lapply(seq_along(co2_path), FUN = function(fileind) {
   
   serial.co2 = NA
 if(length(grep("-14",co2_file[fileind]))>0)
   serial.co2 = "co2_14"
 if(length(grep("-19",co2_file[fileind]))>0)
   serial.co2 = "co2_19"
 if(is.na(serial.co2))
   serial.co2 = "unknown"
 
   read.co2(datafile=co2_path[fileind],
            runname = getrunname(co2_file[fileind]),
            timeaverage = as.numeric(input$user_time_avg)/60,
            location = "mobile",
            splineval= "missing" %in% input$data_options,
            serial = serial.co2)
}) %>% 
   # bind rows 
   bind_rows() %>% 
   
   # average
   # co2.data = co2.data[, lapply(.SD, mean) , 
   #                 by = c("timeint","runname", "location","serial.co2")]

   # set data.table key
   setkey(timeint, runname, location,serial.co2)


# # read in microAeth data
# # [Elena, there doesn't apper to be microAeth data in these data files]
# ae51.data <- lapply(seq_along(ae51_path), FUN = function(fileind) {
#  read.ae51(datafile=ae51_path[fileind],
#            runname = getrunname(ae51_file[fileind]),
#            location = "mobile",
#            timeaverage = as.numeric(input$user_time_avg)/60,
#            splineval= "missing" %in% input$data_options )
# }) %>% 
#    # bind_rows 
#    bind_rows() %>%
# 
#    # # average 
#    # ae51.data = ae51.data[, lapply(.SD, mean) , 
#    #                 by = c("timeint","runname", "location","serial.ae51")]
# 
#    # set key
#    setkey(ae51.data, timeint, runname, location, serial.ae51)

# read in cpc data
cpc.data <- lapply(seq_along(cpc_path), FUN = function(fileind) { 
   read.cpc(datafile = cpc_path[fileind], 
            runname = getrunname(cpc_file[fileind]),
            location = "mobile",
            timeaverage = as.numeric(input$user_time_avg)/60,
            splineval = "missing" %in% input$data_options )
}) %>% 
   # bind rows
   bind_rows() %>%

# # average?
#  cpc.data = cpc.data[, lapply(.SD, mean) , 
#                    by = c("timeint","runname", "location","serial.cpc")]

# set data.table key
setkey(timeint, runname, location, serial.cpc)


# # nano scan data
# # [CZ: keep, no nano scan data?]
# nanoScan.data.list <- lapply(1:length(scan_path), FUN = function(fileind) {
#  temp = read.nano.scan(datafile=scan_path[fileind],
#            runname = getrunname(scan_file[fileind]),
#            timeaverage = as.numeric(input$user_time_avg)/60,
#            location  = "mobile",
#            splineval= "missing" %in% input$data_options)
#    temp
# })
# 
# nanoScan.data = rbindlist(nanoScan.data.list, fill=T)
# nanoScan.data[grep("car", runname, invert=T, ignore.case = T), 
#              runname := paste0(runname, "_Car1")]
#  nanoScan.data = nanoScan.data[, lapply(.SD, mean) , 
#                    by = c("timeint","Status","runname", "location")]
# 
# setkey(nanoScan.data, timeint, runname, location)

```   


# Merge Instrument Data Files

```{r merge.datasets}
#-----merge datasets-----

# create a list of all data to merge 
instrument_list <- list(gps.data, ptrak.data, ptrak.screen.data, cpc.data, co2.data)

# define `merge.all()` function from `merge()`, specifying argument `all = TRUE` 
# because `Reduce()`, used to successively combine elements, does not accept 
# additional arguments
merge.all <- function(x, y){ merge(x, y, all = TRUE) }

# merge all data data sets
instrument_data <- Reduce(merge.all, instrument_list)

# option to make 5-min averages
#output[, min5date := floor_date(timeint, "5 min")]


```

# Get Weather Data

```{r collect.weather.data}
#-----collect weather data-----

# get date range from output using `lubridate` helpers
date_range <- c(start = as.Date(min(instrument_data$timeint) - hours(2)), 
                end = as.Date(max(instrument_data$timeint) +  days(1)) )

# get weather data using customized `riem_measures()` function
weather_data <- get_ASOS(date_start = date_range[["start"]], 
                         date_end = date_range[["end"]], 
                         station = "SEA")

# set data.table keys
#setkey(weather_data, timeint)
#setkey(output, timeint)
#setkey(output, min5)

# # using data.table
# output <- weather_data[instrument_data]

# join weather data and instrument data using dplyr
output <- left_join(instrument_data, weather_data, by = "timeint")

```


# Wranlge Data...

```{r }

# get the names of numeric columns in `output` 
colnamesvals <- names(which(sapply(output, is.numeric)))

# [CZ: I don't know what this is doing - we need some comments or to draft a
# simpler approach]
output[ ,(colnamesvals) := lapply(.SD, as.double), .SDcols = colnamesvals]

output[, (colnamesvals) := lapply(.SD, FUN = function(x){
 tempval= rep(NA, length(x))
 if(is.finite(max(x,na.rm=T))) {
 tempval = na.approx(x, na.rm=F, maxgap= 60/input$user_time_avg*5, rule=1)
 tempval[tempval>=max(x, na.rm=T)]= max(x, na.rm=T)
 tempval[tempval<=min(x, na.rm=T)]= min(x, na.rm=T)
 }
 tempval[is.na(tempval)] = (-9999999)
 tempval[!is.finite(tempval)] = (-9999999)
 tempval
      }),
 .SDcols = colnamesvals, by=runname]

output[output==-9999999] = NA


# identify the maximum time _______ and weather variables
maxgapval <-  1/as.numeric(input$user_time_avg)*60*80
weather_vars <- c("tmpf","relh","drct","sknt","alti","mslp","vsby")

# [CZ: I don't know what this is doing...]
output[, (weather_vars) := lapply(.SD, function(x)
     na.approx(x, na.rm=F, maxgap= maxgapval, rule=1)),
 , .SDcols = weather_vars]



# create a particle number concentration difference value
output[,pnc_diff := pnc_noscreen - pnc_screen]

# try to create pnc_diff/BC ratio column [CZ: NO BC COLUMN]
try(output[,ratio := pnc_diff / BC], silent = TRUE)

# create particle number count background variable
if(input$user_time_avg=="1"){
   
   output[,timeint := as.POSIXct(timeint)]
   # g <- data.table(timeint=seq(min(output$timeint), max(output$timeint), 1))
   # setkey(g, timeint)
   # setkey(output, timeint, runname)
   #
   # output = output[g]
   
   #apply a rolling mean by runname (monitoring period)
   output[, pnc_background :=
          rollapply(pnc_noscreen, width = 30, FUN = function(x){
            quantile(x, 0.05, na.rm=T)
          }, align = 'right', partial = FALSE, fill = NA), by = c("runname")]
 
 output <- output[!is.na(runname), ]
}

```


```{r }

# check the number of rows in `output`
nrow(output[is.na(timeint) | is.na(runname) | is.na(Longitude),])

# drop rows with NAs
output <- output[!is.na(timeint) & !is.na(runname) & !is.na(Longitude)]

# convert columns to numeric [CZ: this did not work...]
#output <- output[, lapply(.SD, makenum), by = c("timeint", "runname", "Longitude")]

# take mean of duplicates
output <- output[, lapply(.SD, mean), by = c("timeint", "runname", "Longitude")]

# number of days
numdays <- unique(format(output$timeint, "%Y-%m-%d"))

# minimum day
mindays = as.POSIXct(min(numdays))

# maximum day
maxdays = as.POSIXct(max(numdays))

# reformat time interval 
output[,day := format(timeint, "%Y-%m-%d")]


# setnames(output, unlist(output[1,]))
# output = output[-1,]


# [Elena, there is no airplane landing data, correct?]
# obsland_dir <- fread("C:\\Users\\elaustin\\OneDrive\\Documents\\UW Postdoc\\MOVUP\\Merged Data\\aircraft landing direction observed.csv")
# 
# obsland_dir[, date := as.POSIXct(date)]
# 
# output[,day := as.POSIXct(day)]
# 
# setkey(obsland_dir, date)
# 
# setkey(output, day)
# 
# output = obsland_dir[output]

```
