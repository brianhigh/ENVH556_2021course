---
title: 'Week 4 Lab: Regression for Prediction'
author: "Lianne Sheppard for ENVH 556"
date: "Winter 2021; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        df_print: "paged"
        toc: true
        toc_depth: 3
        number_sections: true
---

```{r setup, include=FALSE}
#-----setup-----

# set knitr options
knitr::opts_chunk$set(echo = TRUE)

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}
#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, dplyr, tidyr, ggplot2, modelr)

```

```{r directory.organization.read.data, echo=FALSE, warning=FALSE}
#-----directory organization and read data-----

# specify working directory
project_path <- getwd()

# create "Datasets" directory if one does not already exist    
dir.create(file.path(project_path,"Datasets"), showWarnings=FALSE, recursive = TRUE)

# specify data path
data_path <- file.path(project_path,"Datasets")

# specify the file name and path
file_name <- "allseasonsR.rds"
file_path <- file.path(data_path, file_name)

# Download the file if it is not already present
if (!file.exists(file_path)) {
    url <- paste("https://staff.washington.edu/high/envh556/Datasets", 
                 file_name, sep = '/')
    download.file(url = url, destfile = file_path)
}

# Output a warning message if the file cannot be found
if (file.exists(file_path)) {
    snapshot <- readRDS(file_path)
} else warning(paste("Can't find", file_name, "!"))

# remove temporary variables
rm(url, file_name, file_path, data_path)

```



# Purpose

The purpose of this lab is to use principles of “out-of-sample” assessment to
validate regression models.   We will use the snapshot data for model
validation, run a cross-validation, and write a program to more easily repeat
cross-validation procedures. You will use these tools to try to understand the
bias-variance trade-off in these data.


# Getting Started

This section gives some basic R commands for regression, prediction, and model
validation.  We will also learn how to write loops and programs.

## Set-up

* Restrict data to one season:  (fall here) 

```{r fall subset}
#-----fall subset-----

# Traditional base R approach
#fall <- subset(snapshot, season==2)

# Tidyverse approach
fall <- filter(snapshot, season == 2) %>% as_tibble()

```

* Common model names, for later use:

```{r common model covariates}
#-----common model covariates-----

covars_common <- c("D2A1", "A1_50", "A23_400",  "Pop_5000", "D2C", "Int_3000", "D2Comm")

```

## Commands for regression, producing AIC, plotting, and computing prediction R^2^

See also Week 3 lab for these tools and variations of applying them.

* Regression:  

```{r fall regression}
#-----fall regression, common model-----

# build regression formula
frml <- as.formula(paste("ln_nox ~", paste(covars_common, collapse = "+")) )
  
# print fit summary
summary(lm_fall <- lm(frml, data = fall))

```

* AIC and BIC:  

Note, you can provide these functions multiple model objects to facilitate comparisons of AIC or BIC across models fit on the same data.  I only show one model object here.  This has limited utility since there isn't inherent meaning in the AIC or BIC estimates.  Their value is in their comparison across fitted models.

```{r AIC.and.BIC}
#-----AIC and BIC-----

# AIC uses a default penalty of 2
AIC(lm_fall)

# BIC uses a penalty of log(n) where n is the number of observations in the
# dataset
BIC(lm_fall)

```

* Extract predictions, and plot them:  

(Note: the `fig.width` and `fig.height`
options to a chunk give local definitions for the figure height and width.)

```{r predictions.with.modelr, fig.height=5, fig.width=6, warning=FALSE}
#-----predictions with modelr-----

snap2 <- snapshot %>% 
    add_predictions(lm_fall,"preds_fall")  

# specify range
r <- snap2 %>% select(ln_nox, preds_fall) %>% range

# Compare the observations vs. predictions for all seasons, by season 
ggplot(data = snap2, aes(preds_fall, ln_nox)) +
    geom_point(shape = "o", alpha = 0.6) +
    facet_wrap( ~ seasonfac) +
    lims(x = r, y = r) +
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = "Fall model predictions vs. ln(NOx) by season", 
         x = "Predicted ln(NOx) (ln(ppb))",
         y = "Observed ln(NOx) (ln(ppb))",
         caption = "Best fit line is red; 1:1 line is blue") + 
    theme_bw()

```

* Correlations of predictions vs. observations by season:  

(Note:  these are regression-based R^2^ estimates.)

```{r season.specific.correlations}
#-----season-specific correlations-----

pred_summary <- snap2 %>%
    group_by(seasonfac) %>%
    summarise(r = cor(ln_nox, preds_fall), 
              R2 = r^2, 
              .groups = "drop")

```

## Commands for stepwise regression (forward selection)

The R function `step()` allows stepwise regression.  It has forward, backward,
or stepwise regression search algorithms. Forward selection is a useful tool for
ordering a sequence of models based on increasing complexity.

Note:  The `step` function is available in both base R and in the `MASS`
package.  While we won't use it in this lab, the function `addterm()` from the
`MASS` package allows the addition of the next best term to a model.  This
allows use of the F-test or a Chi-square test as alternatives to AIC which is implemented below.

* **Model Set-up**
First define the smallest starting model (the *null* model) and the largest
possible model we will consider (the *full* model). These define the scope.  The
function `step()` has options to specify the direction of the model selection
(we will only use forward selection or `direction = "forward"`), the amount of
output (we'll use `trace = 0` to omit printing any output) and the number of
models fit (the default number of models fit, or `steps`, is 1000).  `step()`
minimizes the AIC and the option `k` controls the degree of penalty.  Since for
our purposes here we don't want to stop early, we use `k = 0` which seems to not
impose a stopping criterion in our dataset.  More investigation is needed to
understand exactly how the `k` option operates w.r.t. stopping the algorithm.


```{r forward.selection.set.up.for.fall.snapshot}
#-----forward selection set-up for fall snapshot-----

# define the smallest model of interest ("null"), an intercept only model here
null <- lm(ln_nox ~ 1, data = fall)

    
# create the largest possible model, a full model that includes all of the
# predictor variables in the dataset.
# Steps:
# 1: get the list of all the potential covariates of interest from the dataset:

#[CZ: Lianne, there are 79 columns in the snapshot data frame, it's not clear
# here if the last five columns are left out intentionally. Ideally, we would 
# figure a way to do this without indexing by position...]
covars_all <- names(fall[12:74])

# 2: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_nox ~ ", paste(covars_all, collapse= "+")))

# # [CZ: Lianne, is there a reason the "full" model was not fit like the null 
# # model (commented out currently)?]
# full <- lm(full, data = fall)

```   

* **Forward Selection**

The first model listed in the command (the "object"), is the initial model used to start the search.  In forward regression we want to start with the smallest model of interest to us, here the intercept only model.  The scope gives the entire range of models by also incorporating the largest possible model.  

```{r fitting.forward.selection}
#-----fitting forward selection model-----

# Note:  k=0 appears to put no restriction on the forward selection and doesn't
# stop until the full model is incorporated. 
# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
forwardreg_fall <- step(null, scope = list(lower = null, upper = full), 
                        trace = 0, direction = "forward", k = 0)

# save the ordered list of names for later use, dropping the intercept
covars_forward <- names(forwardreg_fall$coefficients)[-1]

```

```{r explore.forward.stepwise.results, eval=FALSE}
#-----explore forward stepwise results-----

# structure of the object
str(forwardreg_fall)

# class of the object
class(forwardreg_fall)

# gives the names of the coefficients for use in other work.  Should assign a
# name to it for later use.
# [CZ: we assign the names of this regression in the previous chunk - remove here?]
names(forwardreg_fall$coefficients)

# print a few other things, preceded by what they are in the output (by using
# the cat() command to print text in the output.)

# [CZ: Lianne, not sure exactly what is going on below. At a minimum, the code 
# go should inside the cat() calls? It's unclear where the anova part comes in, 
# perhaps worth some explanation above when the regression is performed?]

cat("AIC:  ", "\n")
forwardreg_fall$anova$AIC
cat("Deviance:  ", "\n")
forwardreg_fall$anova$Deviance
forwardreg_fall$anova$Step
#forwardreg_fall$anova$Df

```


## Manual cross-validation and summarization

Note:  We are summarizing the cross-validation results overall.  This overall
summary is one valid way to do the summary.  Other approaches summarize the
results in each cluster (or cross-validation group) separately and average
these.  (See e.g. the *Introduction to Statistical Learning* textbook:
[ISL](http://www-bcf.usc.edu/~gareth/ISL/) for guidance using this approach.)


```{r manual.CV}
#-----manual CV-----

# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred <- fall %>% mutate(preds = -999)

# loop over the 10 clusters
for (i in 1:10){
  
    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred$cluster == i
    
    # fit the "common" model to the training set by omitting cluster i 
    CV_lm <- lm(frml, data = cv_pred, subset = !is_cluster)
  
    # generate predictions using CV_lm 
    preds <- predict(CV_lm, fall)
    
    # add results to cv_pred dataframe    
    cv_pred$preds[is_cluster] <- preds[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
ln_nox_avg <- mean(cv_pred$ln_nox)

# MSE of predictions
MSE_pred <- mean((cv_pred$ln_nox - cv_pred$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred$ln_nox - ln_nox_avg)^2)

# print the results (rounded)
#paste("RMSE:  ", round( sqrt(MSE_pred), 2))
#paste("MSE-based R2:  ", round( max(1 - MSE_pred/MSE_obs, 0), 2))
# print the results not rounded
paste("RMSE:  ", sqrt(MSE_pred))
paste("MSE-based R2:  ", max(1 - MSE_pred/MSE_obs, 0))  

```


## Functions for cross-validation and MSE 

### Tips on writing functions

From the *R for Data Science* [R4DS](https://r4ds.had.co.nz/index.html) chapter on functions, [Chapter 19](https://r4ds.had.co.nz/functions.html), the
reason to write functions is to automate repetitive tasks and avoid copying and pasting.  This eliminates the need to update the code in multiple places when you make a change and reduces your chances of making mistakes.  A simple rule of thumb is to write a function whenever you have or will need at least three copies of a block of code.

Here are the 3 basic steps to writing a function as described in *R4DS*:

1. Pick a name for the function.
2. List the inputs or *arguments* to the function inside the function call.
3. Put code to accomplish what you want in the body of the function.

Here is a list of twelve best practices for writing functions graciously provided by Brian High:  

1. Use a function when you need to repeat a block of code, instead of simply 
   copying and pasting the same block of code. I.e. "do not repeat yourself".
2. If the code would need to be slightly different with each execution, 
   allow for those differences in the function's argument (parameter) list.
3. All inputs needed for the function should be in the argument list. Do not
   refer to variables within the function which are defined outside of it, but
   were not included in the parameter list. I.e., "make no assumptions".
4. Define default values in the argument list if it makes sense to do so. 
5. The order of arguments should be from most to least essential. If the
    function operates on a data frame, for example, it should be the first 
    argument (this also allows you to use your new function with the pipe 
    operator, `%>%`). Put arguments with defined defaults at the end, so the 
    function user can safely omit providing values for those arguments if they 
    prefer to use the defaults you have defined for them. 6. Use argument names 
    as consistently as possible. Refer to functions you commonly use for 
    examples of consistently named arguments.
7. The function should return a single value or object (e.g., a vector, 
   dataframe, list, etc.).
8. Use the return() function at the end of your function to return this object 
   explicitly. Otherwise, the value of the last expression of the function will 
   be returned implicitly. You will see both methods in practice, and the use 
   of return() varies somewhat by style.
9. Comment your function to make it clear what it expects for input and what
   it returns as output.
10. Write the function to be as resuable (i.e., flexible, generic) as practical.
11. When practical, validate the inputs (argument list) for expected data type 
   and appropriate range if values to avoid strange results if the function is 
   misused.
12. Avoid huge monolithic functions that perform all sorts of tasks. Separate 
    steps into separate functions when it makes sense to do so. This will make 
    your code more reusable, flexible, easier to understand, and therefore 
    easier to debug.


### Our first function:  `get_MSE()`

To start our learning process, first we write a function to estimate the MSE and
MSE-based R^2^.  We call it `get_MSE()`.  Note that in addition to its name and
arguments, there are key formatting details that are required in defining
functions in R.  In particular, pay attention to the arguments inside the
parentheses in the function command, and the use of curly brackets.

```{r define get_MSE}
#-----define get_MSE function-----

# This is a function to get the MSE, RMSE, MSE-based R2
get_MSE <- function(obs,pred) {
    # obs is the outcome variable
    # pred is the prediction from a model
     
    # mean of obs
    obs_avg <- mean(obs)
    
    # MSE of obs (for R2 denominator)
    MSE_obs <- mean((obs-obs_avg)^2)
    
    # MSE of predictions
    MSE_pred <- mean((obs - pred)^2)
    
    # compile output
    result <- c(RMSE = sqrt(MSE_pred),
                MSE_based_R2 = max(1 - MSE_pred / MSE_obs, 0) 
                )
    
    # explicit return (optional)
    return(result)
}

```

Now test our function, using the manual CV results we obtained:

```{r test get_MSE}
#-----test get_MSE-----

get_MSE(cv_pred$ln_nox,cv_pred$preds)

```

### A function to do cross-validation:  do_CV

Now convert our cross-validation to a function:

```{r define.CV.function}
#-----define CV function-----

do_CV <- function(data, group = "group", formula) {
    # In writing functions, it is good practice to:
      # 1. put the dataset first in the function definition
      # 2. put most important arguments first, followed by the specific ones
      # 3. include defaults in the function definitions, where appropriate
    
    # Arguments:
      # data is the data frame
      # group is the grouping variable (in the data frame)
      # formula is the formula to pass to lm
      # the function returns the dataset with a new variable called cvpreds
      # appended to the end; these are the out-of-sample predictions

    # create a variable to store CV predictions 
    CV_pred <- data %>% mutate(cvpreds = -999)
    
    # get the number of distinct groups or clusters
    # Note the use of "[[ ]]" rather than "$" because group is input in the
    # function call as a quoted variable
    k <- length(unique(data[[group]]))
    
    # loop over all the groups or clusters
    for (i in 1:k){
        # define the current group variable as a logical vector
        # again using "[[ ]]" instead of "$"
        is_group <- data[[group]] == i
        
        # subset the data to create the training set by omitting cluster i
        # Note: better to use "!" rather than "-" for negating a logical vector
        training <- data[!is_group, ]
        
        # fit the linear model to the training set
        CV_lm <- lm(formula = formula, data = training)
  
        # generate predictions using CV_lm 
        cvpreds <- predict(CV_lm, data)
        
        # add results to the CV_pred dataframe, for rows of interest
        CV_pred$cvpreds[is_group] <- cvpreds[is_group]
        
    }
    
    # return the dataset
    return(CV_pred)
}

```

Now test the `do_CV` function:

```{r test.do_CV}
#-----test do_CV-----

# run `do_CV()` function
  # Recall we have already specified a common model formula, `frml`
  # Note: the function assumes the group variable is part of the input dataset. 
temp <- do_CV(data = fall, group = "cluster", formula = frml)

# Check to see if result is identical with manual method (done earlier)
identical(cv_pred$preds, temp$cvpreds)

```

And review the CV results:

```{r review.CV.results}
#-----review CV results-----

# now check the results from the CV function
# Summarize with get_MSE
get_MSE(temp$ln_nox, temp$cvpreds)


# get range for plot
r <- temp %>% select(ln_nox, cvpreds) %>% range()

# now look at the scatterplot, observations on the x axis as we typically do for
# considering out-of-sample predictions
ggplot(data = temp, aes(ln_nox, cvpreds)) +
    geom_point(shape = "o", alpha = 0.8) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    labs(title = "Fall model predictions vs. ln(NOx) \nCross-validated", 
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is blue") + 
    theme_bw()

```

## Create 10 CV groups in random order

```{r generate.groups}
#-----generate groups-----

# set the seed to make reproducible
set.seed(283)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(fall)) %>% 
  sample(replace = FALSE)

# now append it to the fall data frame
fall <- mutate(fall, CV_grp = CV_grp)

```

Now test the CV on random groups.  Note that these out-of-sample results should look better.  Why is that the case? (This is a homework challenge question.)

```{r CV.results.on.random.groups}
#-----CV results on random groups-----

# fit the CV with the random groups
temp2 <- do_CV(data = fall, group = 'CV_grp', formula = frml)

# now check the results from the CV function
# Summarize with get_MSE
get_MSE(temp2$ln_nox, temp2$cvpreds)

# now look at the scatterplot, observations on the x axis as we typically do for
# considering out-of-sample predictions
ggplot(data = temp2, aes(ln_nox, cvpreds)) +
    geom_point(shape = "o", alpha = 0.8) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    labs(title = "Fall model predictions vs. ln(NOx) \nCross-validated with random groups", 
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is blue") + 
    theme_bw()

```

## Bias-variance trade-off analysis 

Assignment Request:	Use the order of entry into the stepwise to sequentially
complete the following computations.  For models with one up to the maximum
number of selected variables:  

1. Use the full dataset to obtain in-sample estimates of the RMSE and R2.  
2. Estimate predicted values using cross-validation.  
3.  Compute out-of-sample RMSE and MSE-based R2 estimates.  
 
Steps to coding this:
* compute/retrieve the vector of names output from the forward selection model
* loop over the maximum number of covariates from forward selection
* create a formula that is the linear combination of the first `i` terms of the
forward selection model
* summarize the in-sample prediction MSE and R2
* cross-validate this and summarize the out-of-sample MSE and R2
* store the key set of results, plus the number of covariates in the model and
possibly the variable added to use in plotting
* use ggplot to make the bias-variance trade-off plot


```{r model.order.and.CV, echo = TRUE, warning = FALSE}
#-----model order and CV-----

#get the length of the vector of names from forward selection
n_covars <- length(covars_forward)

# set up an object to store the results in 
# this is better practice
res <- tibble(n_pred = numeric(n_covars),
              covar = character(n_covars),
              in_RMSE = numeric(n_covars),
              in_R2 = numeric(n_covars),
              out_RMSE = numeric(n_covars),
              out_R2 = numeric(n_covars) 
              )

# here is the base R approach.  Note need to use stringsAsFactors = FALSE
#res <-
#    data.frame(
#        n_pred           = numeric(n_covars),
#        covar            = character(n_covars),
#        in_RMSE           = numeric(n_covars),
#        in_R2            = numeric(n_covars),
#        out_RMSE          = numeric(n_covars),
#        out_R2           = numeric(n_covars),
#        stringsAsFactors = FALSE
#    ) 

# loop over n_covars
for (i in 1:n_covars) {
    
    # assign first few variables
    res$n_pred[i] <- i
    res$covar[i] <- covars_forward[i]
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_nox ~ 1 + ", paste(covars_forward[1:i], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = fall) 
    res$in_RMSE[i] <- sqrt(mean(in_model$residuals^2))
    res$in_R2[i] <- summary(in_model)$r.squared
    
    # out-of sample model and estimates
    out_ests <- do_CV(fall, group = 'cluster', fmla)
    out_results <- get_MSE(out_ests$ln_nox, out_ests$cvpreds)
    res$out_RMSE[i] <- out_results[1]
    res$out_R2[i] <- out_results[2]
    
    }

# the data frame res should have all pieces for ggplot


# lapply approach (do not need to pre-assign res)
res <- lapply(1:n_covars, function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_nox ~ 1 + ", paste(covars_forward[1:i], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = fall) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(fall, group = 'cluster', fmla)
    out_results <- get_MSE(out_ests$ln_nox, out_ests$cvpreds)
    
    # compile results
    tibble(n_pred = i,
           covar = covars_forward[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% bind_rows()

```

### Now plot the bias-variance trade-off

Note: the precipitous drop in R2 in the out of sample assessment is best explained by major overfitting of this model after ~ 30 covariates.

```{r bias-var separate plots}
#-----bias-var separate plots-----

# create temporary dataframe for ggplot
temp <- res %>% 
  pivot_longer(cols = c(-n_pred, -covar), 
               names_to = c("Source", ".value"), 
               names_pattern = "(.*)_(.*)" ) 

# R2 plot: with both model based (in sample) R2 and CV generated R2
r2_plot <- ggplot(temp) +
    geom_point(aes(x = n_pred, y = R2, color = Source)) + 
    geom_line(aes(x = n_pred, y = R2, color = Source)) +
    xlab("Model Complexity (# of terms)") +
    ylab(bquote(bold(R ^ 2))) +
    scale_x_continuous(breaks = c(seq(0, 63, 5))) +
    theme_bw() +
    theme(axis.text = element_text(face = "bold"),
          axis.title = element_text(face = "bold")) +
          scale_color_discrete(name = "Prediction\nSource",  
                labels = c("In-sample", "Out-of-sample"))

# show plot
r2_plot


# plot with both model based (in sample) RMSE and CV generated RMSE
rmse_plot <- ggplot(temp) +
    geom_point(aes(x = n_pred, y = RMSE, color = Source)) + 
    geom_line(aes(x = n_pred, y = RMSE, color = Source)) +
    xlab("Model Complexity (# of terms)") +
    ylab(bquote(bold(RMSE))) +
    scale_x_continuous(breaks = c(seq(0, 63, 5))) +
    theme_bw() +
    theme(axis.text = element_text(face = "bold"),
          axis.title = element_text(face = "bold")) +
          scale_color_discrete(name = "Prediction\nSource",  
                labels = c("In-sample", "Out-of-sample"))

# show plot
rmse_plot

```

```{r bias-var combined plots}
#-----bias-var combined plots-----

# create temporary dataframe for plot
temp <- res %>% 
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "in_out_value", 
               values_to = "value" ) %>% 
  separate(col = in_out_value, into = c("Source", "Estimate") )


# plot both model-based (in sample) and CV generated MSE and R2
combined_plot <- ggplot(temp) +
  geom_point(aes(x = n_pred, y = value, color = Source)) + 
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot

```

# Practice Session

This section covers basic practice to be completed during the lab.   We are
going to use the snapshot data described in Mercer et al 2011, discussed in
class, and used last week.  It can be found on the class website.  Note that in
this lab we are treating the errors are independent and identically distributed
even though this assumption is not correct in these data.  (This is the same
assumption typically made in LUR modeling.)

Perform the following tasks:

1. Determine the R project you will use.
2. Explore the dataset a bit, focusing on one particular season.  Make sure you
have some basic understanding of the outcome variable (*ln_nox*), the CV grouping
variable (*cluster*), and the large number of covariates you can use in your prediction modeling.   In
this lab you should restrict your analysis to one season (e.g. fall for the practice session; winter for homework).
3. Fit the model for one of the seasons given in Table 4 of Mercer et al. Make
note of these in-sample estimates of R^2^ and RMSE.
4. Try to manually cross-validate this model using the code given above.
Compare the CV R^2^ and RMSE to your in-sample estimates.  (Note:  Use the cluster
variable in the dataset to define your CV groups.)
5. Use the cross-validation function and repeat your cross-validation analysis.
(If you use the same groups, you should get the same results as in the previous
step.)
6. Make a scatterplot comparing *ln_nox* (the observed dependent variable) on the
x-axis with the cross-validated predictions on the y-axis.  Add the 1:1 line to
your plot. (If you also want to show the best-fit line, you’ll need to put the
predictions on the x-axis rather than the y-axis.)
7. Create your own version of the bias-variance trade-off plot shown in class
using the following steps:
    a. Do a forward stepwise regression of *ln_nox* on a set of plausible variables
with a lax entry criterion (`k=0` in the `step()` function.  (You may restrict
your attention to the list in the forward selection model example given above.)
See the code to learn how to keep track of the order the variables were added.
    b. Use the order of entry into the stepwise to sequentially complete the
following computations.  For models with one up to the maximum number of
selected variables:
        i. Use the full dataset to obtain in-sample estimates of the RMSE and R^2^.
        ii. Estimate predicted values using cross-validation.
        iii. Compute out-of-sample RMSE and MSE-based R^2^ estimates.
    c. In a table or figure(s), summarize the number of variables in each model
along with the R^2^ and/or RMSE estimates from CV and the training data.  If you
choose to show your results in a plot, put the number of variables in the model
on the x-axis vs. the R^2^ or RMSE estimates on the y-axis.  Distinguish the two
kinds of estimates on your plot.  If you choose to show your results in a table,
also include a column for the variable name of the variable added.

# Homework Exercises

1. Write a brief summary of the purpose of the lab and your approach.  Then present your results:

    a.  Describe the results (with appropriate displays in table(s) and/or
    figures(s)), and
    
    b.  Discuss the insights you have obtained from your analyses, both of the
    training data alone and after cross-validation. In your discussion, comment
    on how your in-sample and cross-validated MSE-based R^2^ estimates compare.
    
2. **Extra credit**. Present one or both of the following results in your write-up:

    a.  Repeat the exercise using randomly defined CV groups that ignore the
    gradient clusters.
    
    b.  Repeat the exercise using yet another different set of CV groups, either
    chosen randomly, or spatially, or based on some other criterion.



# Appendix

```{r session.info}
#-----session information----

# print R session information
sessionInfo()

```

```{r code.appendix, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE, , include=TRUE}
#-----code appendix----
```

```{r functions, eval = TRUE}
#-----functions----

# Show the names of all functions defined in the .Rmd
# (e.g. loaded in the environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lapply(c(lsf.str()), getAnywhere)

```
