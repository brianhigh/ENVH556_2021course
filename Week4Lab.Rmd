---
title: 'Week 4 Lab: Regression for Prediction'
author: "Lianne Sheppard for ENVH 556"
date: "Winter 2021; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        df_print: "paged"
        toc: true
        toc_depth: 3
        number_sections: true
---

```{r setup, include=FALSE}
#-----setup-----

# set knitr options
knitr::opts_chunk$set(echo = TRUE)

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}
#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, dplyr, tidyr, purrr, ggplot2, modelr, stringr)

```

```{r directory.organization.read.data, echo=FALSE, warning=FALSE}
#-----directory organization and read data-----

# specify working directory
project_path <- getwd()

# create "Datasets" directory if one does not already exist    
dir.create(file.path(project_path,"Datasets"), showWarnings=FALSE, recursive = TRUE)

# specify data path
data_path <- file.path(project_path,"Datasets")

# specify the file name and path
file_name <- "allseasonsR.rds"
file_path <- file.path(data_path, file_name)

# Download the file if it is not already present
if (!file.exists(file_path)) {
    url <- paste("https://faculty.washington.edu/sheppard/envh556/Datasets", 
                 file_name, sep = '/')
    download.file(url = url, destfile = file_path)
}

# Output a warning message if the file cannot be found
if (file.exists(file_path)) {
    snapshot <- readRDS(file_path)
} else warning(paste("Can't find", file_name, "!"))

# remove temporary variables
rm(url, file_name, file_path, data_path)

```



# Purpose

The purpose of this lab is to use principles of “out-of-sample” assessment to
validate regression models.   We will use the snapshot data for model
validation, run a cross-validation, and write a program to more easily repeat
cross-validation procedures. You will use these tools to try to understand the
bias-variance trade-off in these data.


# Getting Started

This section gives some basic R commands for regression, prediction, and model
validation.  We will also learn how to write loops and programs.

## Set-up

* Restrict data to one season:  (fall here) 

```{r fall subset}
#-----fall subset-----

# Traditional base R approach
#fall <- subset(snapshot, season==2)

# Tidyverse approach
fall <- filter(snapshot, season == 2) %>% as_tibble()

```

* Common model names, for later use:

```{r common model covariates}
#-----common model covariates-----

covars_common <- c("D2A1", "A1_50", "A23_400",  "Pop_5000", "D2C", "Int_3000", "D2Comm")

```

## Commands for regression, producing AIC, plotting, and computing prediction R^2^

See also Week 3 lab for these tools and variations of applying them.

* Regression:  

```{r fall regression}
#-----fall regression, common model-----

# build regression formula
frml <- as.formula(paste("ln_nox ~", paste(covars_common, collapse = "+")) )
  
# print fit summary
summary(lm_fall <- lm(frml, data = fall))

```

* AIC and BIC:  

Note, you can provide these functions multiple model objects to facilitate
comparisons of AIC or BIC across models fit on the same data.  I only show one
model object here.  This has limited utility since there isn't inherent meaning
in the AIC or BIC estimates.  Their value is in their comparison across fitted
models.

```{r AIC.and.BIC}
#-----AIC and BIC-----

# AIC uses a default penalty of 2
AIC(lm_fall)

# BIC uses a penalty of log(n) where n is the number of observations in the
# dataset
BIC(lm_fall)

```

* Extract predictions, and plot them:  

(Note: the `fig.width` and `fig.height`
options to a chunk give local definitions for the figure height and width.)

```{r predictions.with.modelr, warning=FALSE, message=FALSE}
#-----predictions with modelr-----

snap2 <- snapshot %>% 
    add_predictions(lm_fall,"preds_fall")  

# specify range
r <- snap2 %>% select(ln_nox, preds_fall) %>% range

# Compare the observations vs. predictions for all seasons, by season 
ggplot(data = snap2, aes(preds_fall, ln_nox)) +
    geom_point(shape = "o", alpha = 0.6) +
    facet_wrap( ~ seasonfac) +
    lims(x = r, y = r) +
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = "Fall model predictions vs. ln(NOx) by season", 
         x = "Predicted ln(NOx) (ln(ppb))",
         y = "Observed ln(NOx) (ln(ppb))",
         caption = "Best fit line is red; 1:1 line is blue") + 
    theme_bw()

```

* Correlations of predictions vs. observations by season:  

(Note:  these are regression-based R^2^ estimates.)

```{r season.specific.correlations}
#-----season-specific correlations-----

pred_summary <- snap2 %>%
    group_by(seasonfac) %>%
    summarise(r = cor(ln_nox, preds_fall), 
              R2 = r^2, 
              .groups = "drop")

# show summary
pred_summary
```

## Commands for stepwise regression (forward selection)

The R function `step()` allows stepwise regression.  It has forward, backward,
or stepwise regression search algorithms. Forward selection is a useful tool for
ordering a sequence of models based on increasing complexity.

Note:  The `step` function is available in both base R and in the `MASS`
package.  While we won't use it in this lab, the function `addterm()` from the
`MASS` package allows the addition of the next best term to a model.  This
allows use of the F-test or a Chi-square test as alternatives to AIC which is
implemented below.

* **Model Set-up**
First define the smallest starting model (the *null* model) and the largest
possible model we will consider (the *full* model). These define the scope.  The
function `step()` has options to specify the direction of the model selection
(we will only use forward selection or `direction = "forward"`), the amount of
output (we'll use `trace = 0` to omit printing any output) and the number of
models fit (the default number of models fit, or `steps`, is 1000).  `step()`
minimizes the AIC and the option `k` controls the degree of penalty.  Since for
our purposes here we don't want to stop early, we use `k = 0` which seems to not
impose a stopping criterion in our dataset.  More investigation is needed to
understand exactly how the `k` option operates w.r.t. stopping the algorithm.


```{r forward.selection.set.up.for.fall.snapshot}
#-----forward selection set-up for fall snapshot-----

# 1: define the smallest model of interest ("null"), an intercept only model here
null <- lm(ln_nox ~ 1, data = fall)

    
# 2: create the largest possible model, a full model that includes all of the
# predictor variables in the dataset.

# Steps:
# A: get the list of all the potential covariates of interest from the dataset:

# Option 1 (not recommended):  The simple but inelegant way to do this is by
# position.  However this is not a computing best practice because relying on
# position in the data makes assumptions about the data format and risks new
# errors in the future
#covars_all <- names(fall[12:74])

# Option 2 (recommended):  Rely on the beginning of the variable names and use
# these with str_subset in the stringr package:
covars_all <- str_subset(names(snapshot),"Pop_|Int_|Open_|D2|A1_|A23_")

# B: now turn this into a formula for the full model in stepwise regression:
full <- as.formula(paste("ln_nox ~ ", paste(covars_all, collapse= "+")))

# Note:  forward stepwise doesn't need the full model to be fit, only the null
# model (as was done in step 1 above), because that is the "object" that
# represents a model of the appropriate class that is used as the initial model
# in the stepwise search

```   

* **Forward Selection**

The first model listed in the command (the "object"), is the initial model used
to start the search.  In forward regression we want to start with the smallest
model of interest to us, here the intercept only model.  The scope gives the
entire range of models by also incorporating the largest possible model.

```{r fitting.forward.selection}
#-----fitting forward selection model-----

# Note:  k=0 appears to put no restriction on the forward selection and doesn't
# stop until the full model is incorporated. 
# Using k=2 is comparable to standard AIC.
# Using log(n), where n is the number of observations, is comparable to BIC.
forwardreg_fall <- step(null, scope = list(lower = null, upper = full), 
                        trace = 0, direction = "forward", k = 0)

# save the ordered list of names for later use, dropping the intercept
covars_forward <- names(forwardreg_fall$coefficients) %>% setdiff('(Intercept)')

# show the ordered list
covars_forward

```

```{r explore.forward.stepwise.results, eval=FALSE}
#-----explore forward stepwise results-----
# (not shown in ouptput b/c chunk option is eval=FALS)

# structure of the object
str(forwardreg_fall)

# class of the object
class(forwardreg_fall)

# The names of the coefficients in the selected order are stored in
# "forwardfreg_fall$anova$step" and also in "covars_forward".  These vectors are
# not identical because the $anova$step variable has the null model and includes
# "+ ":
cbind(forwardreg_fall$anova$Step[-1],covars_forward)

# print a few other things, preceded by what they are in the output (by using
# the cat() command to print text in the output.)

cat("AIC:  ", "\n")
forwardreg_fall$anova$AIC

cat("Deviance:  ", "\n")
forwardreg_fall$anova$Deviance

```


## Manual cross-validation and summarization

Note:  We are summarizing the cross-validation results overall.  This overall
summary is one valid way to do the summary.  Other approaches summarize the
results in each cluster (or cross-validation group) separately and average
these.  (See e.g. the *Introduction to Statistical Learning* textbook:
[ISL](http://www-bcf.usc.edu/~gareth/ISL/) for guidance using this approach.)


```{r manual.CV}
#-----manual CV-----

# create a numeric variable for CV predictions (using -999 as a placeholder)
cv_pred <- fall %>% mutate(preds = -999)

# loop over the 10 clusters
for (i in 1:10){

    # define the current cluster variable as a logical vector
    is_cluster <- cv_pred$cluster == i

    # fit the "common" model to the training set by omitting cluster i
    CV_lm <- lm(frml, data = cv_pred, subset = !is_cluster)

    # generate predictions using CV_lm
    preds <- predict(CV_lm, fall)

    # add results to cv_pred dataframe
    cv_pred$preds[is_cluster] <- preds[is_cluster]
}

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
ln_nox_avg <- mean(cv_pred$ln_nox)

# MSE of predictions
MSE_pred <- mean((cv_pred$ln_nox - cv_pred$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred$ln_nox - ln_nox_avg)^2)

# # print the results (rounded)
# paste("RMSE:  ", round( sqrt(MSE_pred), 2))
# paste("MSE-based R2:  ", round( max(1 - MSE_pred/MSE_obs, 0), 2))

# print the results not rounded
paste("RMSE:  ", sqrt(MSE_pred))
paste("MSE-based R2:  ", max(1 - MSE_pred/MSE_obs, 0))

```


```{r manual.CV.dplyr}
#-----manual CV using dplyr-----

# Alternative 'tidy' method for manual cross-validation
cv_pred <-
  # do for each cluster in the fall dataset
  lapply(unique(fall$cluster), function(this_cluster){

    # fit the "common" model to the training set lacking this cluster
    CV_lm <- lm(frml, data = filter(fall, cluster != this_cluster))

    # generate predictions for this cluster using training model
    # "." below represents what came before the pipe ("%>%")
    # unname() removes the names attribute created by predict()

    filter(fall, cluster == this_cluster) %>%
      mutate(preds = predict(CV_lm, newdata = .) %>% unname())

    # recombine data from all clusters and sort by ID column
  }) %>% bind_rows() %>% arrange(ID)

# now calculate the MSE, RMSE, MSE-based R2

# mean of observations
ln_nox_avg <- mean(cv_pred$ln_nox)

# MSE of predictions
MSE_pred <- mean((cv_pred$ln_nox - cv_pred$preds)^2)

# MSE of observations (for R2 denominator)
MSE_obs <- mean((cv_pred$ln_nox - ln_nox_avg)^2)

# # print the results (rounded)
# paste("RMSE:  ", round( sqrt(MSE_pred), 2))
# paste("MSE-based R2:  ", round( max(1 - MSE_pred/MSE_obs, 0), 2))

# print the results not rounded
paste("RMSE:  ", sqrt(MSE_pred))
paste("MSE-based R2:  ", max(1 - MSE_pred/MSE_obs, 0))  

```


## Functions for cross-validation and MSE 

### Tips on writing functions

From the *R for Data Science* [R4DS](https://r4ds.had.co.nz/index.html) chapter
on functions, [Chapter 19](https://r4ds.had.co.nz/functions.html), the reason to
write functions is to automate repetitive tasks and avoid copying and pasting.
This eliminates the need to update the code in multiple places when you make a
change and reduces your chances of making mistakes.  A simple rule of thumb is
to write a function whenever you have or will need at least three copies of a
block of code.

Here are the 3 basic steps to writing a function as described in *R4DS*:

1. Pick a name for the function.
2. List the inputs or *arguments* to the function inside the function call.
3. Put code to accomplish what you want in the body of the function.

Here is a list of twelve best practices for writing functions graciously
provided by Brian High:

1. Use a function when you need to repeat a block of code, instead of simply 
   copying and pasting the same block of code. I.e. "do not repeat yourself".
2. If the code would need to be slightly different with each execution, 
   allow for those differences in the function's argument (parameter) list.
3. All inputs needed for the function should be in the argument list. Do not
   refer to variables within the function which are defined outside of it, but
   were not included in the parameter list. I.e., "make no assumptions".
4. Define default values in the argument list if it makes sense to do so. 
5. The order of arguments should be from most to least essential. If the
   function operates on a data frame, for example, it should be the first 
   argument (this also allows you to use your new function with the pipe 
   operator, `%>%`). Put arguments with defined defaults at the end, so the 
   function user can safely omit providing values for those arguments if they 
   prefer to use the defaults you have defined for them. 
6. Use argument names as consistently as possible. Refer to functions you
   commonly use for examples of consistently named arguments.
7. The function should return a single value or object (e.g., a vector, 
   dataframe, list, etc.).
8. Use the return() function at the end of your function to return this object 
   explicitly. Otherwise, the value of the last expression of the function will 
   be returned implicitly. You will see both methods in practice, and the use 
   of return() varies somewhat by style.
9. Comment your function to make it clear what it expects for input and what
   it returns as output.
10. Write the function to be as resuable (i.e., flexible, generic) as practical.
11. When practical, validate the inputs (argument list) for expected data type 
    and appropriate range if values to avoid strange results if the function is 
    misused.
12. Avoid huge monolithic functions that perform all sorts of tasks. Separate 
    steps into separate functions when it makes sense to do so. This will make 
    your code more reusable, flexible, easier to understand, and therefore 
    easier to debug.


### Our first function:  `get_MSE()`

To start our learning process, first we write a function to estimate the MSE and
MSE-based R^2^.  We call it `get_MSE()`.  Note that in addition to its name and
arguments, there are key formatting details that are required in defining
functions in R.  In particular, pay attention to the arguments inside the
parentheses in the function command, and the use of curly brackets.

```{r define get_MSE}
#-----define get_MSE function-----

# This is a function to get the MSE, RMSE, MSE-based R2
get_MSE <- function(obs,pred) {
    # obs is the outcome variable
    # pred is the prediction from a model
     
    # mean of obs
    obs_avg <- mean(obs)
    
    # MSE of obs (for R2 denominator)
    MSE_obs <- mean((obs-obs_avg)^2)
    
    # MSE of predictions
    MSE_pred <- mean((obs - pred)^2)
    
    # compile output
    result <- c(RMSE = sqrt(MSE_pred),
                MSE_based_R2 = max(1 - MSE_pred / MSE_obs, 0) 
                )
    
    # explicit return (optional)
    return(result)
}

```

Now test our function, using the manual CV results we obtained:

```{r test get_MSE}
#-----test get_MSE-----

get_MSE(cv_pred$ln_nox,cv_pred$preds)

```

### A function to do cross-validation:  do_CV

Now convert our cross-validation to a function:

```{r define.CV.function}
#-----define CV function-----

do_CV <- function(data, id = "id", group = "group", formula) {
  # In writing functions, it is good practice to:
  # 1. put the dataset first in the function definition
  # 2. put most important arguments first, followed by the specific ones
  # 3. include defaults in the function definitions, where appropriate

  # Arguments:
  # data is the data frame
  # id is the unique variable for determining sort order of data frame
  # group is the grouping variable (a variable in the data frame)
  # formula is the formula to pass to lm
  # the function returns the dataset with a new variable called cvpreds
  # appended to the end; these are the out-of-sample predictions

  # do for each cluster in the  dataset
  # (Note the use of "[[ ]]" rather than "$" because group is input in the
  # function call as a quoted variable)
  lapply(unique(data[[group]]), function(this_group){
    
    # fit the "common" model to the training set (without this group)
    CV_lm <- lm(formula, data = data[data[[group]] != this_group,])
    
    # generate predictions for this group using training model
    data[data[[group]] == this_group,] %>%
      mutate(cvpreds = predict(CV_lm, newdata = .) %>% unname())
    
    # recombine data from all clusters and sort by ID column
    # note use of ".data[[ ]]" to return the value of variable id
  }) %>% bind_rows() %>% arrange(.data[[id]])
  
  # return the dataset (the last-evaluated object is always returned by default)
}

```

Now test the `do_CV` function:

```{r test.do_CV}
#-----test do_CV-----

# run `do_CV()` function
  # Recall we have already specified a common model formula, `frml`
  # Note: the function assumes the group variable is part of the input dataset. 
temp <- do_CV(data = fall, id = 'ID', group = "cluster", formula = frml)

# Check to see if result is same as manual method (done earlier)
all.equal(cv_pred$preds, temp$cvpreds)

```

And review the CV results:

```{r review.CV.results}
#-----review CV results-----

# now check the results from the CV function
# Summarize with get_MSE
get_MSE(temp$ln_nox, temp$cvpreds)


# get range for plot
r <- temp %>% select(ln_nox, cvpreds) %>% range()

# now look at the scatterplot, observations on the x axis as we typically do for
# considering out-of-sample predictions
ggplot(data = temp, aes(ln_nox, cvpreds)) +
    geom_point(shape = "o", alpha = 0.8) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    labs(title = "Fall model predictions vs. ln(NOx) \nCross-validated", 
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is blue") + 
    theme_bw()

```

## Create 10 CV groups in random order

```{r generate.groups}
#-----generate groups-----

# set the seed to make reproducible
set.seed(283)

# create vector of CV groups
  # first generate a vector with values 1:10 equal to the number of dataset rows
  # then randomize (with argument replace = FALSE)
CV_grp <- rep(1:10, length.out = nrow(fall)) %>% 
  sample(replace = FALSE)

# now append it to the fall data frame
fall <- mutate(fall, CV_grp = CV_grp)

```

Now test the CV on random groups.  Note that these out-of-sample results should
look better.  Why is that the case? (This is a homework challenge question.)

```{r CV.results.on.random.groups}
#-----CV results on random groups-----

# fit the CV with the random groups
temp2 <- do_CV(data = fall, id = 'ID', group = 'CV_grp', formula = frml)

# now check the results from the CV function
# Summarize with get_MSE
get_MSE(temp2$ln_nox, temp2$cvpreds)

# now look at the scatterplot, observations on the x axis as we typically do for
# considering out-of-sample predictions
ggplot(data = temp2, aes(ln_nox, cvpreds)) +
    geom_point(shape = "o", alpha = 0.8) +
    lims(x= r, y = r) +
    coord_fixed() +
    geom_abline(intercept = 0, slope = 1, color = "blue") +
    labs(title = "Fall model predictions vs. ln(NOx) \nCross-validated with random groups", 
         x = "Observed ln(NOx) (ln(ppb))",
         y = "Predicted ln(NOx) (ln(ppb))",
         caption = "1:1 line is blue") + 
    theme_bw()

```

## Bias-variance trade-off analysis 

Assignment Request:	Use the order of entry into the stepwise to sequentially
complete the following computations.  For models with one up to the maximum
number of selected variables:  

1. Use the full dataset to obtain in-sample estimates of the RMSE and R2.  
2. Estimate predicted values using cross-validation.  
3.  Compute out-of-sample RMSE and MSE-based R2 estimates.  
 
Steps to coding this:  
* compute/retrieve the vector of names output from the forward selection model  
* loop over the maximum number of covariates from forward selection  
* create a formula that is the linear combination of the first `i` terms of the
forward selection model   
* summarize the in-sample prediction MSE and R2   
* cross-validate this and summarize the out-of-sample MSE and R2   
* store the key set of results, plus the number of covariates in the model and
possibly the variable added to use in plotting   
* use ggplot to make the bias-variance trade-off plot  


```{r model.order.and.CV, echo = TRUE, warning = FALSE}
#-----model order and CV-----

# Create a tibble `res` with the results from multiple cross-validation steps.
# This tibble should have all pieces for ggplot.  The following code uses an
# lapply approach, for which the `res` object does not need to be pre-assigned
# to be an empty formatted object

# apply along length of the vector of names from forward selection
res <- lapply(seq_along(covars_forward), function(i){
    
    # define the formula, updated to add a term each time
    fmla <- as.formula(
        paste("ln_nox ~ 1 + ", paste(covars_forward[seq_len(i)], collapse = "+"))) 
    
    # in-sample model and estimates
    in_model <- lm(fmla, data = fall) 
    
    # out-of sample model and estimates
    out_ests <- do_CV(fall, id = "ID", group = 'cluster', fmla)
    out_results <- get_MSE(out_ests$ln_nox, out_ests$cvpreds)
    
    # compile results
    tibble(n_pred = i,
           covar = covars_forward[i],
           in_RMSE = sqrt(mean(in_model$residuals^2)),
           in_R2 = summary(in_model)$r.squared,
           out_RMSE = out_results[["RMSE"]],
           out_R2 = out_results[["MSE_based_R2"]] 
           )
    
    }) %>% 
  
    # bind list rows together
    bind_rows()

```

### Now plot the bias-variance trade-off

Note: the precipitous drop in R2 in the out of sample assessment is best
explained by major overfitting of this model after ~ 30 covariates.

```{r bias-var separate plots}
#-----bias-var separate plots-----

# create temporary dataframe for ggplot
temp <- res %>% 
  pivot_longer(cols = c(-n_pred, -covar), 
               names_to = c("Source", ".value"), 
               names_pattern = "(.*)_(.*)" ) 

# R2 plot: with both model based (in sample) R2 and CV generated R2
r2_plot <- ggplot(temp) +
    geom_point(aes(x = n_pred, y = R2, color = Source)) + 
    geom_line(aes(x = n_pred, y = R2, color = Source)) +
    xlab("Model Complexity (# of terms)") +
    ylab(bquote(bold(R ^ 2))) +
    scale_x_continuous(breaks = c(seq(0, 63, 5))) +
    theme_bw() +
    theme(axis.text = element_text(face = "bold"),
          axis.title = element_text(face = "bold")) +
          scale_color_discrete(name = "Prediction\nSource",  
                labels = c("In-sample", "Out-of-sample"))

# show plot
r2_plot


# plot with both model based (in sample) RMSE and CV generated RMSE
rmse_plot <- ggplot(temp) +
    geom_point(aes(x = n_pred, y = RMSE, color = Source)) + 
    geom_line(aes(x = n_pred, y = RMSE, color = Source)) +
    xlab("Model Complexity (# of terms)") +
    ylab(bquote(bold(RMSE) (ln(ppb)) )) +
    scale_x_continuous(breaks = c(seq(0, 63, 5))) + 
    theme_bw() +
    theme(axis.text = element_text(face = "bold"),
          axis.title = element_text(face = "bold")) +
          scale_color_discrete(name = "Prediction\nSource",  
                labels = c("In-sample", "Out-of-sample"))

# show plot
rmse_plot

```

The next step puts both plots together.  We also restrict the range on the y-axis so that the plots are more informative visually.

```{r bias-var combined plots}
#-----bias-var combined plots-----

# Note that the out-of-sample R2 for a model with 31 terms is about .7; we can
# find the RMSE using res$out_RMSE[31]:
max(res$out_RMSE[1:31])
min(res$out_RMSE[1:31])

# This suggests limits for the R2 plot of ~.5-.7 to 1 and limits for the RMSE
# plot of 0 - ~.5 or so

# Note: it is very hard to "zoom in" on a particular facet (i.e. restrict the
# axis limits), so one strategy is to omit the values above a certain level to
# achieve the desired axis limits. Expanding the axis limits of a facet can be 
# achieved with `geom_blank()`. An alternative would be to create the separate 
# plots, specifying the axis limits if desired, then combining them with 
# `egg::ggarrange()` or similar.

# specify y axis limit for RMSE 
y_lim <- 0.5

# create temporary dataframe for plot
temp <- res %>% 
  
  # make long dataframe
  pivot_longer(cols = c(ends_with("_RMSE"), ends_with("_R2")), 
               names_to = "Source_Estimate", 
               values_to = "value" ) %>%
  
  # separate the "Source" column for in and out of sample
  separate(col = Source_Estimate, into = c("Source", "Estimate") ) %>% 

  # set high RMSE values to NA, then filter out these values before plotting
  mutate(value = ifelse(Estimate == "RMSE" & value > y_lim, NA, value)) %>%
  filter(!is.na(value))

# plot both model-based (in sample) and CV generated MSE and R2
# filter out rows NA for plot_value column to prevent ggplot error messages
combined_plot <- ggplot(temp) +
  geom_point(aes(x = n_pred, y = value, color = Source)) +
  geom_line(aes(x = n_pred, y = value, color = Source)) +
  xlab("Model Complexity (# of terms)") +
  ylab("") +
  scale_x_continuous(breaks = c(seq(0, 63, 5))) +
  facet_wrap(~ Estimate, scales = "free_y", ncol = 1, strip.position = "right") +
  theme_bw() 

#show plot
combined_plot

```

# Practice Session

This section covers basic practice to be completed during the lab.   We are
going to use the snapshot data described in [Mercer et al,
2011](https://doi.org/10.1016/j.atmosenv.2011.05.043), discussed in class, and
used last week.  It can also be found on the class website.  Note that in this
lab we are treating the errors are independent and identically distributed even
though this assumption is not correct in these data.  (This is the same
assumption typically made in LUR modeling.)

Perform the following tasks:

1. Determine the R project you will use.
2. Explore the dataset a bit, focusing on one particular season.  Make sure you
have some basic understanding of the outcome variable (*ln_nox*), the CV
grouping variable (*cluster*), and the large number of covariates you can use in
your prediction modeling.   In this lab you should restrict your analysis to one
season (e.g. fall for the practice session; winter for homework).
3. Fit the model for one of the seasons given in Table 4 of Mercer et al. Make
note of these in-sample estimates of R^2^ and RMSE.
4. Try to manually cross-validate this model using the code given above.
Compare the CV R^2^ and RMSE to your in-sample estimates.  (Note:  Use the cluster
variable in the dataset to define your CV groups.)
5. Use the cross-validation function and repeat your cross-validation analysis.
(If you use the same groups, you should get the same results as in the previous
step.)
6. Make a scatterplot comparing *ln_nox* (the observed dependent variable) on the
x-axis with the cross-validated predictions on the y-axis.  Add the 1:1 line to
your plot. (If you also want to show the best-fit line, you’ll need to put the
predictions on the x-axis rather than the y-axis.)
7. Create your own version of the bias-variance trade-off plot shown in class
using the following steps:
    a. Do a forward stepwise regression of *ln_nox* on a set of plausible variables
with a lax entry criterion (`k=0`) in the `step()` function.  (You may restrict
your attention to the list in the forward selection model example given above.)
See the code to learn how to keep track of the order the variables were added.
    b. Use the order of entry into the stepwise to sequentially complete the
following computations.  For models with one up to the maximum number of
selected variables:
        i. Use the full dataset to obtain in-sample estimates of the RMSE and R^2^.
        ii. Estimate predicted values using cross-validation.
        iii. Compute out-of-sample RMSE and MSE-based R^2^ estimates.
    c. In a table or figure(s), summarize the number of variables in each model
along with the R^2^ and/or RMSE estimates from CV and the training data.  If you
choose to show your results in a plot, put the number of variables in the model
on the x-axis vs. the R^2^ or RMSE estimates on the y-axis.  Distinguish the two
kinds of estimates on your plot.  If you choose to show your results in a table,
also include a column for the variable name of the variable added.

# Homework Exercises

1. Write a brief summary of the purpose of the lab and your approach.  Then
present your results:

    a.  Describe the results (with appropriate displays in table(s) and/or
    figures(s)), and
    
    b.  Discuss the insights you have obtained from your analyses, both of the
    training data alone and after cross-validation. In your discussion, comment
    on how your in-sample and cross-validated MSE-based R^2^ estimates compare.
    
2. **Extra credit**. Present one or both of the following results in your
write-up:

    a.  Repeat the exercise using randomly defined CV groups that ignore the
    gradient clusters.
    
    b.  Repeat the exercise using yet another different set of CV groups, either
    chosen randomly, or spatially, or based on some other criterion.  Make sure
    you describe your choice and scientific rationale.



# Appendix

```{r session.info}
#-----session information----

# print R session information
sessionInfo()

```

```{r code.appendix, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE, , include=TRUE}
#-----code appendix----
```

```{r functions, eval = TRUE}
#-----functions----

# Show the names of all functions defined in the .Rmd
# (e.g. loaded in the environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lsf.str() %>% set_names() %>% map(get, .GlobalEnv)

```
